{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, Activation\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import optimizers\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the pretrained word embeddings. We will use GloVe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use this dictionary to store every vector corresponding to every word.\n",
    "embeddings_dict = {}\n",
    "n_words_model = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the text file, all the values are separated by a space. \n",
    "#The first value is the word, and the next 300 values are the vectors components\n",
    "with open(\"glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2559e-01  1.3630e-02  1.0306e-01 -1.0123e-01  9.8128e-02  1.3627e-01\n",
      " -1.0721e-01  2.3697e-01  3.2870e-01 -1.6785e+00  2.2393e-01  1.2409e-01\n",
      " -8.6708e-02  3.3010e-01  3.4375e-01 -8.7582e-04 -2.9658e-01  2.4417e-01\n",
      " -1.1592e-01 -3.5742e-02 -1.0830e-02  2.0776e-01  2.9285e-01 -7.3491e-02\n",
      " -1.8598e-01 -2.0090e-01 -9.5366e-02  6.3732e-03 -1.3620e-01  9.2028e-02\n",
      " -3.9957e-02  1.9027e-01 -1.0456e-01  2.7670e-03 -7.1742e-01 -1.2915e-01\n",
      " -1.3451e-03  2.7002e-01 -5.3023e-02  2.2148e-01  1.3881e-01 -1.5051e-01\n",
      " -1.9150e-01  1.6402e-01  9.7484e-02  5.6841e-02  3.9789e-01  4.0725e-01\n",
      "  1.4802e-01  2.1569e-01 -1.0671e-01 -1.0232e-01  2.4810e-02 -2.2100e-01\n",
      " -1.0720e-02  1.4234e-01 -2.8242e-01  1.9254e-01  8.6720e-02 -3.8970e-01\n",
      "  1.1321e-01  1.3779e-03  6.4009e-03 -1.6206e-01 -8.2153e-02 -5.5397e-01\n",
      "  3.6789e-01 -4.0159e-03  2.0710e-01 -3.7157e-01  2.5135e-01 -1.9544e-01\n",
      " -4.7059e-02  1.7155e-01 -2.4036e-01 -4.6086e-02  1.9429e-01 -1.8939e-01\n",
      " -7.1974e-03  6.9481e-02  5.9175e-02 -1.7585e-01  1.0653e-01  1.6933e-01\n",
      " -3.6122e-02  2.9911e-02 -1.1830e-01  1.3916e-01 -3.7951e-02  1.0690e-01\n",
      " -2.6069e-01 -1.0307e-01 -1.2272e-01 -1.5032e-01 -4.2409e-02  1.3354e-02\n",
      " -2.8510e-01  1.1248e-02  1.6073e-01 -1.6384e-01  2.1233e-01 -1.8476e-01\n",
      " -9.0874e-04  6.6687e-02  1.6918e-01 -3.5004e-01  9.9016e-02  4.6393e-01\n",
      " -1.9462e-01  1.0346e-01 -2.5668e-01 -3.6516e-01 -1.8963e-01 -2.1933e-01\n",
      "  2.4634e-02  6.5627e-02 -1.1120e-01 -1.6400e-01  1.0874e-02 -8.4688e-02\n",
      " -1.4923e-01 -7.0223e-02  2.8887e-02  8.3497e-02 -1.6193e-02 -2.4926e-03\n",
      "  1.7186e-01  9.8749e-03  8.0237e-02  1.4774e-01  4.3206e-02  2.7716e-01\n",
      "  5.7697e-01 -4.1297e-02  1.2765e-01 -9.1517e-02  1.4132e-01  8.7579e-02\n",
      "  9.3224e-02  1.5346e-02 -1.9856e-01  1.7277e-02 -1.0708e-01 -1.3059e-02\n",
      " -3.7227e-01  7.8568e-02  1.6677e-01 -1.5359e-01 -3.3294e-01  3.6986e-02\n",
      "  1.1697e-01  3.9781e-02  3.8464e-02 -1.6247e-01  4.1280e-01 -7.7491e-02\n",
      "  4.5490e-02  1.1330e-01  8.2177e-03 -2.5052e-01  7.0966e-02 -1.1388e-01\n",
      " -1.1503e-01 -1.1014e-01  1.0499e-01  1.5878e-01 -2.7023e-01 -1.1006e-02\n",
      "  7.6057e-04  3.3902e-01  2.5564e-01  1.6342e-01 -5.6019e-01  1.3055e-01\n",
      "  7.6311e-02 -2.8334e-02  2.8721e-01 -2.7844e-02 -1.1561e-01  3.4925e-01\n",
      " -1.2420e-01  2.1405e-01  2.4116e-01 -3.1343e-02  1.0913e-01 -2.4755e-01\n",
      " -4.5429e-02 -8.2178e-02 -1.8831e-01  1.8446e-01 -9.7074e-02  3.2395e-01\n",
      "  1.0658e-01 -2.6676e-01 -2.7311e-01  1.7181e-02  2.5796e-01 -2.8048e-01\n",
      "  3.0790e-01 -2.1800e-01  8.7415e-01 -1.2297e-01  1.0991e-01 -2.9797e-01\n",
      "  1.3394e-01  1.0615e-01 -1.0789e-01 -3.5976e-01 -1.8311e-01 -4.5133e-01\n",
      "  3.4967e-02 -1.9847e-01  2.1965e-01  8.1520e-02  2.5810e-01  4.0173e-02\n",
      "  3.1394e-02  1.9069e-01  7.5800e-02 -6.0638e-02  2.0739e-01  9.8390e-03\n",
      " -2.6930e-01  6.6515e-02 -1.0711e-01  5.9916e-03  2.3284e-01 -5.8663e-02\n",
      "  9.8993e-02 -8.1464e-02  6.7004e-02 -1.4305e-01  2.5506e-01 -3.1971e-01\n",
      " -3.1070e-02 -9.2451e-02  2.9440e-01  2.8947e-01 -5.9804e-02  2.4286e-01\n",
      " -1.6755e-01  4.2031e-02  5.1261e-01  2.4525e-01 -6.5983e-01  6.2456e-02\n",
      "  5.2204e-02 -2.5717e-02 -8.0613e-02  8.0869e-02  2.2821e-01 -1.0217e-01\n",
      " -2.0719e-01 -1.2123e-02  3.4916e-01  8.6527e-02  6.6288e-02 -9.9828e-02\n",
      "  2.5843e-01  1.1943e-01 -1.3667e-01 -4.3962e-01  2.3704e-01  3.1296e-02\n",
      "  7.4701e-02 -2.2387e-01  7.8162e-03 -1.9016e-01  4.4444e-02  2.0191e-01\n",
      " -2.0814e-01 -2.8382e-01  1.0427e-01 -2.1098e-01  1.8865e-01  3.1659e-01\n",
      " -2.0753e+00 -7.1045e-02  5.2419e-01  5.6023e-02 -2.5295e-01 -6.2168e-02\n",
      " -1.0989e-01 -3.5755e-01 -7.9244e-02  3.7472e-01 -2.8353e-01  1.6337e-01\n",
      "  1.1165e-01 -9.8002e-02  6.0148e-02 -1.5619e-01 -1.1949e-01  2.3445e-01\n",
      "  8.1367e-02  2.4618e-01 -1.5242e-01 -3.4224e-01 -2.2394e-02  1.3684e-01]\n"
     ]
    }
   ],
   "source": [
    "#Let's test our word embedding\n",
    "print(embeddings_dict[\".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.90358085e-06 -1.19096007e-03 -1.49708314e-03  6.39434550e-05\n",
      "   3.04065780e-04  6.07081989e-04  5.52570342e-05 -1.06642593e-04\n",
      "  -9.29549562e-05  7.59098021e-04 -1.61068822e-03  6.14387690e-04\n",
      "   5.36723573e-05 -1.30443975e-03 -3.71704130e-03  1.02124550e-03\n",
      "   2.22533149e-04  7.55519701e-05 -1.81694651e-04 -7.04529735e-04\n",
      "   7.12508029e-06  8.49930481e-04 -1.59072001e-04  1.26696970e-03\n",
      "  -3.89776249e-04 -1.46072554e-03 -2.71664648e-03  7.91806018e-04\n",
      "   3.32597377e-04  2.52611574e-04  7.92106451e-04 -1.67142961e-03\n",
      "  -9.94131422e-04 -2.05603628e-04 -1.79913989e-03  4.66575134e-04\n",
      "  -4.39789617e-05 -2.35717970e-04  5.35240166e-04 -1.86112347e-03\n",
      "   1.59427703e-03  2.09764197e-03  1.77891201e-03  7.66572549e-04\n",
      "   1.19911454e-03  3.44824744e-04  1.62335578e-03 -1.63695676e-03\n",
      "   8.71035814e-04 -1.47054823e-03 -8.91756397e-04  1.99355673e-04\n",
      "  -1.85216027e-04 -2.27524520e-04  2.23924952e-03 -9.07293329e-04\n",
      "  -1.07500311e-03 -7.82842649e-04  1.07751712e-03 -3.47122656e-04\n",
      "   6.19850797e-04  1.29995257e-03 -1.34298058e-03  6.39994559e-04\n",
      "  -2.27441712e-04 -6.56445604e-04 -5.39692043e-04 -1.10564310e-03\n",
      "   4.98225465e-04  6.90902201e-05  6.26638421e-04  8.64602389e-04\n",
      "   1.64357131e-03  5.81795298e-04  1.36319857e-03  2.64766318e-04\n",
      "   8.97671942e-04 -1.04157708e-03 -4.58222850e-04 -6.66560343e-04\n",
      "   1.11479365e-04  7.74823308e-04  1.16367393e-03 -5.09810526e-04\n",
      "   6.40620439e-04 -8.72961874e-04 -2.81993309e-04 -7.89234149e-04\n",
      "   1.22620063e-03  2.71299113e-06 -8.20054336e-05 -1.05190922e-04\n",
      "  -9.20770680e-04  2.54150475e-04 -1.50845996e-03  3.58745537e-04\n",
      "  -7.12315935e-04 -2.15450218e-03 -4.86782396e-04 -7.48284742e-05\n",
      "  -6.98057089e-04  5.01191582e-04 -2.14952368e-04  3.98545405e-04\n",
      "  -8.56940809e-05  1.57726754e-03  4.96815229e-04 -1.02076960e-03\n",
      "   1.52870112e-03  1.48565896e-03 -4.18861355e-04  5.94001308e-04\n",
      "   2.61400456e-04 -1.07492319e-03  9.93609263e-04 -8.81895077e-04\n",
      "  -7.30183375e-04  1.08538630e-03  2.34926754e-04  4.73660583e-04\n",
      "   3.99936190e-04 -7.56605316e-05  2.29538228e-04  1.73015292e-03\n",
      "  -9.07482187e-04 -8.36228361e-04 -2.60378842e-04  3.83877926e-04\n",
      "   2.93326108e-04  4.77142978e-04  1.57123747e-03 -8.93693850e-05\n",
      "  -1.59110393e-04 -1.02048007e-03  1.37498911e-03  1.13965572e-03\n",
      "  -1.73301462e-04 -1.72828666e-03  1.17941695e-03  1.48676104e-03\n",
      "   8.17720877e-04 -1.13810671e-06 -1.38899929e-03 -1.90806439e-03\n",
      "   7.69295339e-05 -1.34658422e-03 -2.91427887e-04  1.40152346e-03\n",
      "  -1.40379912e-03  2.11284003e-04 -4.12281628e-04  8.40774262e-04\n",
      "  -3.66986964e-04 -9.31620154e-04 -9.18712814e-07  1.55928086e-03\n",
      "   2.05544937e-03  1.77203354e-03 -4.23438904e-04  8.89383987e-04\n",
      "   3.16170950e-03  8.13624309e-04 -1.00769310e-03  4.75082954e-04\n",
      "   1.61759552e-03  4.78978067e-04 -1.01634276e-03  6.86258480e-04\n",
      "  -4.36328801e-06 -2.01493873e-03 -9.24737362e-04 -1.12516192e-03\n",
      "   2.85142325e-04 -7.72910696e-05 -1.71592664e-03  1.26742365e-04\n",
      "  -1.40801007e-04  2.96670319e-04 -6.14714469e-04  1.18811307e-04\n",
      "   1.24741218e-03 -3.62018936e-05 -1.17217094e-03  1.22334660e-04\n",
      "  -1.69308099e-03  5.57930252e-04 -8.16986684e-04  2.62431045e-06\n",
      "   1.21465826e-03 -5.43231360e-04  7.11381220e-04 -3.06686360e-04\n",
      "  -2.36686835e-05  4.91054719e-04 -1.16136762e-03 -1.14739194e-03\n",
      "  -2.02949959e-04  6.62114660e-04 -6.62251726e-05  1.24451387e-04\n",
      "  -3.77676743e-04 -4.25580890e-04 -4.03087245e-04  1.10563645e-03\n",
      "  -1.04709736e-03 -1.60955487e-03 -6.96619549e-04 -1.29139444e-03\n",
      "  -1.26557328e-03 -2.99430654e-05  9.20569444e-05 -1.31863354e-03\n",
      "  -4.42121190e-04  7.97562648e-04  9.38345814e-04 -7.86232530e-04\n",
      "   1.18868232e-03 -2.67588113e-05  1.05747115e-04  2.24076938e-03\n",
      "   5.40281295e-04  4.79001197e-04  9.34287039e-04  8.36732203e-05\n",
      "  -1.12445804e-03  6.81975695e-04 -8.29494640e-04 -5.49437663e-04\n",
      "   8.41798410e-05  8.46709596e-04  8.16911370e-04 -8.22667606e-04\n",
      "  -1.40722577e-04  1.89015643e-03 -1.71699758e-03 -4.56944984e-04\n",
      "  -7.90703393e-04 -6.45356089e-04 -1.32595358e-03 -9.94930110e-04\n",
      "  -1.10266863e-03  1.62629128e-03  1.04705930e-03  5.16300386e-04\n",
      "   8.81028023e-04 -7.62201139e-04 -1.76888546e-03 -7.70401048e-04\n",
      "   5.31573467e-04 -7.17518484e-04  7.66204587e-04  1.48291778e-03\n",
      "  -1.06337797e-03  1.70982955e-03  1.43080940e-03  3.51738564e-04\n",
      "  -4.31923081e-04  2.03592459e-03 -1.26395581e-03  1.90421786e-03\n",
      "   2.90545117e-04 -9.62201415e-04 -6.80804638e-04  9.89314943e-04\n",
      "  -2.65100872e-03  2.41974840e-05  2.32326947e-04  2.36139223e-03\n",
      "   1.76907281e-04 -1.14420384e-03 -7.05395647e-04 -5.92135703e-04\n",
      "   1.87732729e-03  2.01524172e-03  1.29179201e-03 -2.02538955e-05\n",
      "  -7.27548974e-04  1.39346685e-04 -1.52973421e-04  4.70038178e-04\n",
      "   1.66577571e-03 -1.23030811e-06  3.07230672e-04  1.00059224e-03\n",
      "  -9.05429925e-04  1.86712098e-04 -4.74625380e-04 -1.10362842e-03\n",
      "   1.88726989e-03  4.13364167e-04  4.18924307e-04 -7.45056983e-04\n",
      "   7.61989106e-04 -4.86171618e-04  9.35475039e-04  1.10980347e-03\n",
      "  -1.58599491e-03 -9.73214824e-04 -4.19281792e-04 -9.12621817e-04]]\n"
     ]
    }
   ],
   "source": [
    "#Define a word embedding for unknown word with random values\n",
    "embeddings_dict['UNK'] = np.random.randn(1,300)*0.001\n",
    "#embeddings_dict['UNK'] = embeddings_dict['UNK']/(embeddings_dict['UNK']**2)\n",
    "\n",
    "print(embeddings_dict['UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dict = {'yelp':   'sentiment_analysis/yelp_labelled.txt',\n",
    "                 'amazon': 'sentiment_analysis/amazon_cells_labelled.txt',\n",
    "                 'imdb':   'sentiment_analysis/imdb_labelled.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence    Wow... Loved this place.\n",
      "label                              1\n",
      "source                          yelp\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] = source  # Add another column filled with the source name\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = df_list[0]\n",
    "df_amazon = df_list[1]\n",
    "df_imdb = df_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label  source\n",
       "0    So there is no way for me to plug it in here i...      0  amazon\n",
       "1                          Good case, Excellent value.      1  amazon\n",
       "2                               Great for the jawbone.      1  amazon\n",
       "3    Tied to charger for conversations lasting more...      0  amazon\n",
       "4                                    The mic is great.      1  amazon\n",
       "..                                                 ...    ...     ...\n",
       "995  The screen does get smudged easily because it ...      0  amazon\n",
       "996  What a piece of junk.. I lose more calls on th...      0  amazon\n",
       "997                       Item Does Not Match Picture.      0  amazon\n",
       "998  The only thing that disappoint me is the infra...      0  amazon\n",
       "999  You can not answer calls with the unit, never ...      0  amazon\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a function to vectorize our data:\n",
    "\n",
    "def text_to_matrix(text,n_words, n_features):\n",
    "    # Input\n",
    "    ## text : is a string with the paragraph or text that needs to be vectorized\n",
    "    # Output\n",
    "    ## vector: is a matrix of dimensions (150,300), 150 words by 300 features.\n",
    "    \n",
    "    #First we need to separate our text by spaces\n",
    "    vector_list = []\n",
    "    tokenizer = nltk.tokenize.word_tokenize\n",
    "    text = tokenizer(text)\n",
    "    \n",
    "    for word in text:\n",
    "        if(word!=''):\n",
    "            try: \n",
    "                vector_list.append(embeddings_dict[word.lower()].reshape(1,n_features))\n",
    "            except:\n",
    "                vector_list.append(embeddings_dict['UNK'].reshape(1,n_features))\n",
    "        if(len(vector_list)==n_words):\n",
    "            break \n",
    "    zero_padding = np.zeros((1,n_features))\n",
    "    for i in range(n_words-len(vector_list)):\n",
    "        vector_list.append(zero_padding)\n",
    "        \n",
    "    vector = np.asarray(vector_list)\n",
    "    return vector\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test of the text_to_matrix function \n",
    "a = text_to_matrix(\"hi, my name is Josue. Why aren't you talking to me.\",n_words_model,300)\n",
    "a = np.reshape(a,(a.shape[0],300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 300)\n",
      "[[ 0.40838    -0.18427999 -0.17573    ... -0.52645999  0.81630999\n",
      "   0.74274999]\n",
      " [-0.25538999 -0.25723001  0.13169    ... -0.23289999 -0.12226\n",
      "   0.35499001]\n",
      " [-0.22746    -0.13658001 -0.38997    ... -0.18444    -0.38227999\n",
      "   0.55346   ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to vectorize all the dataset\n",
    "def vectorize_dataset(df, n_words, n_features):\n",
    "    y = df['label'].to_numpy()\n",
    "    X = df['sentence'].to_numpy()\n",
    "    X = np.reshape(X, (X.shape[0], 1))\n",
    "    #f = lambda x: text_to_matrix(str(x),150,300)\n",
    "    #func = np.vectorize(f)\n",
    "    #x = func(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y= vectorize_dataset(df_amazon, n_words_model,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_examples = []\n",
    "for text in X:\n",
    "    list_examples.append(text_to_matrix(str(text),n_words_model,300))\n",
    "X = np.asarray(list_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 30, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=X.reshape(1000,n_words_model,300)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y= Y.reshape(1000,1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create our model\n",
    "def lstm_model(LSTM_hidden_units):\n",
    "    \n",
    "    input_layer = Input(shape=(n_words_model,300), dtype= \"float32\")\n",
    "    \n",
    "    X = LSTM(units = LSTM_hidden_units, return_sequences= True)(input_layer)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = LSTM(units = LSTM_hidden_units, return_sequences = False)(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    \n",
    "  \n",
    "    X = Dense(1)(X)\n",
    "    out = Activation('sigmoid')(X)\n",
    "   \n",
    "    model = Model(inputs= input_layer, outputs=out)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_model(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 30, 300)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 64)            93440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 126,529\n",
      "Trainable params: 126,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples\n",
      "Epoch 1/120\n",
      "900/900 [==============================] - 4s 4ms/sample - loss: 0.6918 - binary_accuracy: 0.5467\n",
      "Epoch 2/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.6678 - binary_accuracy: 0.5944\n",
      "Epoch 3/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.5767 - binary_accuracy: 0.7189\n",
      "Epoch 4/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.5104 - binary_accuracy: 0.7689\n",
      "Epoch 5/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.4547 - binary_accuracy: 0.7867\n",
      "Epoch 6/120\n",
      "900/900 [==============================] - 1s 911us/sample - loss: 0.4127 - binary_accuracy: 0.8256\n",
      "Epoch 7/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.3828 - binary_accuracy: 0.8467\n",
      "Epoch 8/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.3256 - binary_accuracy: 0.8756\n",
      "Epoch 9/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.2945 - binary_accuracy: 0.8956 0s - loss: 0.2996 - binary_accuracy: 0.8\n",
      "Epoch 10/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.2901 - binary_accuracy: 0.8889\n",
      "Epoch 11/120\n",
      "900/900 [==============================] - 1s 833us/sample - loss: 0.3883 - binary_accuracy: 0.8211\n",
      "Epoch 12/120\n",
      "900/900 [==============================] - 1s 865us/sample - loss: 0.2783 - binary_accuracy: 0.9022\n",
      "Epoch 13/120\n",
      "900/900 [==============================] - 1s 980us/sample - loss: 0.2118 - binary_accuracy: 0.9322\n",
      "Epoch 14/120\n",
      "900/900 [==============================] - 1s 860us/sample - loss: 0.1867 - binary_accuracy: 0.9422\n",
      "Epoch 15/120\n",
      "900/900 [==============================] - 1s 854us/sample - loss: 0.1876 - binary_accuracy: 0.9444\n",
      "Epoch 16/120\n",
      "900/900 [==============================] - 1s 834us/sample - loss: 0.1374 - binary_accuracy: 0.9611\n",
      "Epoch 17/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.1290 - binary_accuracy: 0.9656\n",
      "Epoch 18/120\n",
      "900/900 [==============================] - 1s 972us/sample - loss: 0.1454 - binary_accuracy: 0.9600\n",
      "Epoch 19/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.1240 - binary_accuracy: 0.9689\n",
      "Epoch 20/120\n",
      "900/900 [==============================] - 1s 821us/sample - loss: 0.1677 - binary_accuracy: 0.9500\n",
      "Epoch 21/120\n",
      "900/900 [==============================] - 1s 840us/sample - loss: 0.1654 - binary_accuracy: 0.9444\n",
      "Epoch 22/120\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.1548 - binary_accuracy: 0.958 - 1s 778us/sample - loss: 0.1543 - binary_accuracy: 0.9589\n",
      "Epoch 23/120\n",
      "900/900 [==============================] - 1s 889us/sample - loss: 0.1110 - binary_accuracy: 0.9711\n",
      "Epoch 24/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.1056 - binary_accuracy: 0.9667\n",
      "Epoch 25/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0852 - binary_accuracy: 0.9722\n",
      "Epoch 26/120\n",
      "900/900 [==============================] - 1s 888us/sample - loss: 0.0990 - binary_accuracy: 0.9733\n",
      "Epoch 27/120\n",
      "900/900 [==============================] - 1s 832us/sample - loss: 0.0900 - binary_accuracy: 0.9711\n",
      "Epoch 28/120\n",
      "900/900 [==============================] - 1s 869us/sample - loss: 0.1091 - binary_accuracy: 0.9667\n",
      "Epoch 29/120\n",
      "900/900 [==============================] - 1s 841us/sample - loss: 0.1082 - binary_accuracy: 0.9622\n",
      "Epoch 30/120\n",
      "900/900 [==============================] - 1s 864us/sample - loss: 0.0905 - binary_accuracy: 0.9678\n",
      "Epoch 31/120\n",
      "900/900 [==============================] - 1s 851us/sample - loss: 0.1089 - binary_accuracy: 0.9656\n",
      "Epoch 32/120\n",
      "900/900 [==============================] - 1s 843us/sample - loss: 0.0791 - binary_accuracy: 0.9767\n",
      "Epoch 33/120\n",
      "900/900 [==============================] - 1s 903us/sample - loss: 0.1327 - binary_accuracy: 0.9433\n",
      "Epoch 34/120\n",
      "900/900 [==============================] - 1s 964us/sample - loss: 0.0959 - binary_accuracy: 0.9667\n",
      "Epoch 35/120\n",
      "900/900 [==============================] - 1s 867us/sample - loss: 0.1400 - binary_accuracy: 0.9511\n",
      "Epoch 36/120\n",
      "900/900 [==============================] - 1s 838us/sample - loss: 0.0963 - binary_accuracy: 0.9689\n",
      "Epoch 37/120\n",
      "900/900 [==============================] - 1s 867us/sample - loss: 0.1768 - binary_accuracy: 0.9311\n",
      "Epoch 38/120\n",
      "900/900 [==============================] - 1s 873us/sample - loss: 0.2451 - binary_accuracy: 0.9078\n",
      "Epoch 39/120\n",
      "900/900 [==============================] - 1s 865us/sample - loss: 0.1332 - binary_accuracy: 0.9444\n",
      "Epoch 40/120\n",
      "900/900 [==============================] - 1s 814us/sample - loss: 0.1197 - binary_accuracy: 0.9500\n",
      "Epoch 41/120\n",
      "900/900 [==============================] - 1s 831us/sample - loss: 0.1031 - binary_accuracy: 0.9611\n",
      "Epoch 42/120\n",
      "900/900 [==============================] - 1s 816us/sample - loss: 0.0678 - binary_accuracy: 0.9822\n",
      "Epoch 43/120\n",
      "900/900 [==============================] - 1s 751us/sample - loss: 0.0489 - binary_accuracy: 0.9833\n",
      "Epoch 44/120\n",
      "900/900 [==============================] - 1s 758us/sample - loss: 0.0476 - binary_accuracy: 0.9856\n",
      "Epoch 45/120\n",
      "900/900 [==============================] - 1s 788us/sample - loss: 0.0346 - binary_accuracy: 0.9878\n",
      "Epoch 46/120\n",
      "900/900 [==============================] - 1s 851us/sample - loss: 0.0333 - binary_accuracy: 0.9889\n",
      "Epoch 47/120\n",
      "900/900 [==============================] - 1s 851us/sample - loss: 0.0286 - binary_accuracy: 0.9889\n",
      "Epoch 48/120\n",
      "900/900 [==============================] - 1s 824us/sample - loss: 0.0290 - binary_accuracy: 0.9889\n",
      "Epoch 49/120\n",
      "900/900 [==============================] - 1s 798us/sample - loss: 0.0261 - binary_accuracy: 0.9889\n",
      "Epoch 50/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0248 - binary_accuracy: 0.9889\n",
      "Epoch 51/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0288 - binary_accuracy: 0.9900\n",
      "Epoch 52/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0232 - binary_accuracy: 0.9900\n",
      "Epoch 53/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0237 - binary_accuracy: 0.9878\n",
      "Epoch 54/120\n",
      "900/900 [==============================] - 1s 846us/sample - loss: 0.0258 - binary_accuracy: 0.9911\n",
      "Epoch 55/120\n",
      "900/900 [==============================] - 1s 834us/sample - loss: 0.0212 - binary_accuracy: 0.9900\n",
      "Epoch 56/120\n",
      "900/900 [==============================] - 1s 797us/sample - loss: 0.0225 - binary_accuracy: 0.9911\n",
      "Epoch 57/120\n",
      "900/900 [==============================] - 1s 846us/sample - loss: 0.0216 - binary_accuracy: 0.9900\n",
      "Epoch 58/120\n",
      "900/900 [==============================] - 1s 853us/sample - loss: 0.0216 - binary_accuracy: 0.9911\n",
      "Epoch 59/120\n",
      "900/900 [==============================] - 1s 830us/sample - loss: 0.0210 - binary_accuracy: 0.9889\n",
      "Epoch 60/120\n",
      "900/900 [==============================] - 1s 746us/sample - loss: 0.0232 - binary_accuracy: 0.9911\n",
      "Epoch 61/120\n",
      "900/900 [==============================] - 1s 752us/sample - loss: 0.0216 - binary_accuracy: 0.9911\n",
      "Epoch 62/120\n",
      "900/900 [==============================] - 1s 786us/sample - loss: 0.0214 - binary_accuracy: 0.9889\n",
      "Epoch 63/120\n",
      "900/900 [==============================] - 1s 781us/sample - loss: 0.0211 - binary_accuracy: 0.9900\n",
      "Epoch 64/120\n",
      "900/900 [==============================] - 1s 764us/sample - loss: 0.0213 - binary_accuracy: 0.9911\n",
      "Epoch 65/120\n",
      "900/900 [==============================] - 1s 796us/sample - loss: 0.0210 - binary_accuracy: 0.9900s - loss: 0.0059 - binary_accuracy:\n",
      "Epoch 66/120\n",
      "900/900 [==============================] - 1s 765us/sample - loss: 0.0211 - binary_accuracy: 0.9911\n",
      "Epoch 67/120\n",
      "900/900 [==============================] - 1s 875us/sample - loss: 0.0209 - binary_accuracy: 0.9911\n",
      "Epoch 68/120\n",
      "900/900 [==============================] - 1s 801us/sample - loss: 0.0204 - binary_accuracy: 0.9889\n",
      "Epoch 69/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0220 - binary_accuracy: 0.9911\n",
      "Epoch 70/120\n",
      "900/900 [==============================] - 1s 906us/sample - loss: 0.0198 - binary_accuracy: 0.9900\n",
      "Epoch 71/120\n",
      "900/900 [==============================] - 1s 728us/sample - loss: 0.0203 - binary_accuracy: 0.9911\n",
      "Epoch 72/120\n",
      "900/900 [==============================] - 1s 731us/sample - loss: 0.0219 - binary_accuracy: 0.9900\n",
      "Epoch 73/120\n",
      "900/900 [==============================] - 1s 716us/sample - loss: 0.0187 - binary_accuracy: 0.9911\n",
      "Epoch 74/120\n",
      "900/900 [==============================] - 1s 785us/sample - loss: 0.0209 - binary_accuracy: 0.9900\n",
      "Epoch 75/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0199 - binary_accuracy: 0.9889\n",
      "Epoch 76/120\n",
      "900/900 [==============================] - 1s 926us/sample - loss: 0.0192 - binary_accuracy: 0.9900\n",
      "Epoch 77/120\n",
      "900/900 [==============================] - 1s 923us/sample - loss: 0.0199 - binary_accuracy: 0.9889\n",
      "Epoch 78/120\n",
      "900/900 [==============================] - 1s 837us/sample - loss: 0.0203 - binary_accuracy: 0.9900\n",
      "Epoch 79/120\n",
      "900/900 [==============================] - 1s 792us/sample - loss: 0.0197 - binary_accuracy: 0.9878\n",
      "Epoch 80/120\n",
      "900/900 [==============================] - 1s 965us/sample - loss: 0.0200 - binary_accuracy: 0.9900\n",
      "Epoch 81/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0202 - binary_accuracy: 0.9911\n",
      "Epoch 82/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0189 - binary_accuracy: 0.9900\n",
      "Epoch 83/120\n",
      "900/900 [==============================] - 1s 821us/sample - loss: 0.0194 - binary_accuracy: 0.9900\n",
      "Epoch 84/120\n",
      "900/900 [==============================] - 1s 962us/sample - loss: 0.0189 - binary_accuracy: 0.9911\n",
      "Epoch 85/120\n",
      "900/900 [==============================] - 1s 863us/sample - loss: 0.0203 - binary_accuracy: 0.9911\n",
      "Epoch 86/120\n",
      "900/900 [==============================] - 1s 809us/sample - loss: 0.0198 - binary_accuracy: 0.9911\n",
      "Epoch 87/120\n",
      "900/900 [==============================] - 1s 915us/sample - loss: 0.0194 - binary_accuracy: 0.9900\n",
      "Epoch 88/120\n",
      "900/900 [==============================] - 1s 903us/sample - loss: 0.0201 - binary_accuracy: 0.9900\n",
      "Epoch 89/120\n",
      "900/900 [==============================] - 1s 925us/sample - loss: 0.0201 - binary_accuracy: 0.9911\n",
      "Epoch 90/120\n",
      "900/900 [==============================] - 1s 854us/sample - loss: 0.0187 - binary_accuracy: 0.9911\n",
      "Epoch 91/120\n",
      "900/900 [==============================] - 1s 921us/sample - loss: 0.0189 - binary_accuracy: 0.9889\n",
      "Epoch 92/120\n",
      "900/900 [==============================] - 1s 918us/sample - loss: 0.0197 - binary_accuracy: 0.9900s - loss: 0.0035 - binary_accura\n",
      "Epoch 93/120\n",
      "900/900 [==============================] - 1s 910us/sample - loss: 0.0187 - binary_accuracy: 0.9911\n",
      "Epoch 94/120\n",
      "900/900 [==============================] - 1s 901us/sample - loss: 0.0196 - binary_accuracy: 0.9911\n",
      "Epoch 95/120\n",
      "900/900 [==============================] - 1s 943us/sample - loss: 0.0194 - binary_accuracy: 0.9889\n",
      "Epoch 96/120\n",
      "900/900 [==============================] - 1s 795us/sample - loss: 0.0188 - binary_accuracy: 0.9911\n",
      "Epoch 97/120\n",
      "900/900 [==============================] - 1s 911us/sample - loss: 0.0198 - binary_accuracy: 0.9911\n",
      "Epoch 98/120\n",
      "900/900 [==============================] - 1s 921us/sample - loss: 0.0188 - binary_accuracy: 0.9922\n",
      "Epoch 99/120\n",
      "900/900 [==============================] - 1s 809us/sample - loss: 0.0192 - binary_accuracy: 0.9900\n",
      "Epoch 100/120\n",
      "900/900 [==============================] - 1s 750us/sample - loss: 0.0188 - binary_accuracy: 0.9900\n",
      "Epoch 101/120\n",
      "900/900 [==============================] - 1s 870us/sample - loss: 0.0187 - binary_accuracy: 0.9889\n",
      "Epoch 102/120\n",
      "900/900 [==============================] - 1s 754us/sample - loss: 0.0191 - binary_accuracy: 0.9911\n",
      "Epoch 103/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0187 - binary_accuracy: 0.9911\n",
      "Epoch 104/120\n",
      "900/900 [==============================] - 1s 797us/sample - loss: 0.0197 - binary_accuracy: 0.9900\n",
      "Epoch 105/120\n",
      "900/900 [==============================] - 1s 785us/sample - loss: 0.0189 - binary_accuracy: 0.9911\n",
      "Epoch 106/120\n",
      "900/900 [==============================] - 1s 769us/sample - loss: 0.0186 - binary_accuracy: 0.9900\n",
      "Epoch 107/120\n",
      "900/900 [==============================] - 1s 782us/sample - loss: 0.0181 - binary_accuracy: 0.9911\n",
      "Epoch 108/120\n",
      "900/900 [==============================] - 1s 775us/sample - loss: 0.0193 - binary_accuracy: 0.9900\n",
      "Epoch 109/120\n",
      "900/900 [==============================] - 1s 769us/sample - loss: 0.0188 - binary_accuracy: 0.9911\n",
      "Epoch 110/120\n",
      "900/900 [==============================] - 1s 765us/sample - loss: 0.0187 - binary_accuracy: 0.9922\n",
      "Epoch 111/120\n",
      "900/900 [==============================] - 1s 801us/sample - loss: 0.0183 - binary_accuracy: 0.9889\n",
      "Epoch 112/120\n",
      "900/900 [==============================] - 1s 780us/sample - loss: 0.0192 - binary_accuracy: 0.9900\n",
      "Epoch 113/120\n",
      "900/900 [==============================] - 1s 919us/sample - loss: 0.0185 - binary_accuracy: 0.9911\n",
      "Epoch 114/120\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.0186 - binary_accuracy: 0.9922\n",
      "Epoch 115/120\n",
      "900/900 [==============================] - 1s 849us/sample - loss: 0.0195 - binary_accuracy: 0.9911\n",
      "Epoch 116/120\n",
      "900/900 [==============================] - 1s 822us/sample - loss: 0.0194 - binary_accuracy: 0.9900\n",
      "Epoch 117/120\n",
      "900/900 [==============================] - 1s 809us/sample - loss: 0.0196 - binary_accuracy: 0.9889\n",
      "Epoch 118/120\n",
      "900/900 [==============================] - 1s 785us/sample - loss: 0.0195 - binary_accuracy: 0.9900\n",
      "Epoch 119/120\n",
      "900/900 [==============================] - 1s 966us/sample - loss: 0.0188 - binary_accuracy: 0.9889\n",
      "Epoch 120/120\n",
      "900/900 [==============================] - 1s 782us/sample - loss: 0.0184 - binary_accuracy: 0.9911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a0acb9cfd0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 120, batch_size = 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c5f4ebe4391c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review\n"
     ]
    }
   ],
   "source": [
    "x_test1=  text_to_matrix(\"I really enjoyed my time with her\",n_words_model,300)\n",
    "x_test1 = np.reshape(x_test1,(1,x_test1.shape[0],300))\n",
    "if(model.predict(x_test1)>0.5):\n",
    "    print(\"Positive review\")\n",
    "else:\n",
    "    print(\"Negative review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
